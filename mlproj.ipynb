{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juliacsds/CityScape/blob/main/mlproj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydshh4JZhV4v"
      },
      "source": [
        "# Predicting The Cost of a One-Bedroom Apartment in Major World Cities\n",
        "\n",
        "First, let's read in our dataset. This was obtained from: https://www.kaggle.com/datasets/mvieira101/global-cost-of-living?resource=download\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "DTOrxT4ahP5m",
        "outputId": "70f39fdc-a869-4820-d96f-0f27bc3ed972"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aecd2426-ce3b-492a-9028-1d28c8796381\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aecd2426-ce3b-492a-9028-1d28c8796381\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving cost-of-living_v2.csv to cost-of-living_v2 (1).csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns; sns.set()\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('cost-of-living_v2.csv')\n",
        "data.columns = ['city', 'country', 'meal_inexpensive', 'meal_midrange',\n",
        "                'mcdonalds',\n",
        "                'beer_domestic_restaurant', 'beer_imported_restaurant',\n",
        "                'capuccino', 'coke', 'water_restaurant', 'milk', 'bread',\n",
        "                'rice', 'eggs', 'cheese', 'chicken_fillet', 'beef', 'apple',\n",
        "                'banana', 'orange', 'tomato', 'potato', 'onion', 'lettuce',\n",
        "                'water_market', 'wine', 'beer_domestic_market',\n",
        "                'beer_imported_market', 'cigarettes', 'one_way_ticket',\n",
        "                'monthly_pass', 'taxi_start', 'taxi_1km', 'taxi_hour',\n",
        "                'gasoline', 'volkswagen_new', 'toyota_new', 'utilities_apt',\n",
        "                'mobile_tariff', 'internet', 'fitness_club', 'tennis_court',\n",
        "                'cinema', 'preschool_private', 'international_primary',\n",
        "                'jeans', 'summer_dress', 'nike_shoes', 'leather_shoes',\n",
        "                'apt_1bed_center', 'apt_1bed_outside', 'apt_3bed_center',\n",
        "                'apt_3bed_outside', 'apt_center_sqm_price',\n",
        "                'apt_outside_sqm_price', 'avg_monthly_salary',\n",
        "                'mortgage_interest', 'data_quality']\n",
        "print('Total Rows of Data:', len(data))\n",
        "data.head()"
      ],
      "metadata": {
        "id": "zOGpDicbUk3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEg8IqN7lk4J"
      },
      "source": [
        "# Data Preparation and Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkCg2UrjqPhf"
      },
      "source": [
        "First, let's examine the overall features and missing values in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoIkDfaGjNVB"
      },
      "outputs": [],
      "source": [
        "missing = data.isnull().sum()\n",
        "missing_sorted = missing.sort_values(ascending=False)\n",
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "plt.title('Number of Missing Values per Feature')\n",
        "plt.barh(missing_sorted.index, missing_sorted.values)\n",
        "plt.ylabel('Feature')\n",
        "plt.xlabel('Number of Missing Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Remx5oaxqWvw"
      },
      "source": [
        "We are trying to predict the **\"apt_1bed_center\"** feature, which is the price\n",
        "of a one-bedroom apartment in the city center. So we can drop the features that are too closely correlated to our target feature--namely, the price of a 3\n",
        "bedroom apartment inside our outside the city center, the price of a 1\n",
        "bedroom apartment outside the city center, and the average price per square\n",
        "meter of apartments inside or outside the city center. These are all too similar to what we're predicting."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropping these values is also good because they happen to be missing a lot of data in the original dataset. There are other features such as **\"international_primary\", \"monthly_pass\", and \"tennis_court\"** that also have high volumes of missing data that we might want to look out for when testing models, as if they aren't important features than dropping them could provide us with more data to work with."
      ],
      "metadata": {
        "id": "bw1a7GY5Wle9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains a \"data quality\" column that identifies which records are considered to have good data quality by the creator. However, only keeping those rows for training yields just 744 records for training and testing, which is 15% of the original dataset size.\n",
        "\n",
        "Thus, to have more data, we will focus on first dropping the unnecessary columns and then removing records with missing values, without worrying about data quality. We will test models on both this dataset and a smaller datset with only \"good quality\" records.\n",
        "\n",
        "We can drop any rows with a missing value for any one of the features to clean up our data. This leaves us with 1427 records in the dataset to use for training/testing."
      ],
      "metadata": {
        "id": "N7oX7DrbWqzX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJOhU5ZClxbU"
      },
      "outputs": [],
      "source": [
        "# # For later use\n",
        "# data_good_qual = data[data['data_quality'] != 0]\n",
        "# Clean data\n",
        "data_clean = data.drop(['apt_1bed_outside', 'apt_3bed_outside', 'apt_3bed_center',\n",
        "                  'apt_center_sqm_price', 'apt_outside_sqm_price'], axis=1)\n",
        "data_clean.dropna(inplace=True)\n",
        "data_clean.drop_duplicates(inplace=True)\n",
        "data_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6njACKlyfwi"
      },
      "source": [
        "Next, we can look at which features are most closely correlated with the target variable (price of a 1 bed apartment in the city center)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsUYizp_u95Y"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 30))\n",
        "plt.subplots_adjust(hspace=0.2)\n",
        "# plt.suptitle(\"Correlations\")\n",
        "\n",
        "for i in range(3):\n",
        "  cur_method = 'pearson'\n",
        "  if i == 1:\n",
        "    cur_method = 'spearman'\n",
        "  elif i == 2:\n",
        "    cur_method = 'kendall'\n",
        "  corr = data_clean.corr(method=cur_method).abs()\n",
        "  corr_y = corr['apt_1bed_center'].sort_values(ascending=False)\n",
        "\n",
        "  ax = plt.subplot(3,1,i+1)\n",
        "  ax.barh(corr_y.index, corr_y.values)\n",
        "  ax.set_xlabel('Correlation Values')\n",
        "  ax.set_ylabel('Features')\n",
        "  ax.set_title('Correlation with Price of Apartment (' + cur_method + ')')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "6aajiJcwZROV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK6Di-OQzUaZ"
      },
      "source": [
        "Based on these plots, average monthly salary, meal prices, and cinema prices seem to individually have a bigger correlation with our target feature."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(1,3,1)\n",
        "x = data_clean['avg_monthly_salary'].values\n",
        "y = data_clean['apt_1bed_center'].values\n",
        "plt.scatter(x,y)\n",
        "a, b = np.polyfit(x,y, 1)\n",
        "plt.plot(x, a*x+b, color='red', linestyle='--')\n",
        "plt.ylabel('1 bed Apt Price')\n",
        "plt.xlabel('Avg Monthly Salary')\n",
        "plt.title('Apt Price vs. Avg Monthly Salary')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "x = data_clean['meal_midrange'].values\n",
        "plt.scatter(x,y)\n",
        "a, b = np.polyfit(x,y, 1)\n",
        "plt.plot(x, a*x+b, color='red', linestyle='--')\n",
        "plt.xlabel('Midrange Meal Price')\n",
        "plt.yticks([])\n",
        "plt.title('Apt Price vs. Midrange Meal Price')\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "x = data_clean['cinema'].values\n",
        "plt.scatter(x,y)\n",
        "a, b = np.polyfit(x,y, 1)\n",
        "plt.plot(x, a*x+b, color='red', linestyle='--')\n",
        "plt.xlabel('Cinema Price')\n",
        "plt.yticks([])\n",
        "plt.title('Apt Price vs. Cinema Price')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sWlhiWqlX3wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsKxWVOe4rzM"
      },
      "source": [
        "What does the distribution of our target feature look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqL-rYOJ06oP"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "plt.subplot(3,1,1)\n",
        "data_sorted_apt = data_clean.sort_values(by=['apt_1bed_center'])\n",
        "plt.scatter(data_sorted_apt.index, data_sorted_apt['apt_1bed_center'], s=4)\n",
        "plt.xlabel('City')\n",
        "plt.ylabel('1 bed Apt Price')\n",
        "plt.title(\"Scatterplot of Apartment Prices\")\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "plt.hist(data_clean['apt_1bed_center'], bins=40)\n",
        "plt.xlabel('1 bed Apt Prices')\n",
        "plt.ylabel('# of Cities')\n",
        "plt.title(\"Histogram of Apartment Prices\")\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "plt.boxplot(data_clean['apt_1bed_center'])\n",
        "plt.ylabel('1 bed Apt Price')\n",
        "plt.title(\"Boxplot of Apartment Prices\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection (Data Cleaning + Exploration)\n",
        "\n",
        "While we've removed the missing values, there could be outliers in the data and removing them would be helpful for testing our models. We can clearly see from the plots above that there are some outliers that have extremely high values for the apartment price variable, which should probably be removed. We run anomaly detection with the isolation forest algorithm to find and specifically identify outliers like this. This algorithm is good for our purposes because it is fast, not susceptible to curse of dimensionality (we have many features in our dataset), and it does not require significant tuning of hyperparameters (only the anomaly threshold value to identify an anomaly)."
      ],
      "metadata": {
        "id": "fJJmrHBsYc5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "isof = IsolationForest()\n",
        "isof.fit(data_clean[['apt_1bed_center']])\n",
        "data_clean['anomaly_scores'] = isof.decision_function(\n",
        "    data_clean[['apt_1bed_center']]\n",
        "    )\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.hist(data_clean['anomaly_scores'])\n",
        "plt.xlabel('Anomaly Score')\n",
        "plt.ylabel('Number of records')\n",
        "plt.title('Anomaly Scores Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w3dMpKRnYmim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create column for identifying anomalies."
      ],
      "metadata": {
        "id": "xOSYEnDmYyKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean['anomaly'] = 1"
      ],
      "metadata": {
        "id": "FmGaOUUfY2wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We identify anomalies by using a cutoff score to identify normal vs. non-anomalous objects."
      ],
      "metadata": {
        "id": "FjDRAe7eY5aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cutoff = -0.15\n",
        "anomalies = data_clean[data_clean['anomaly_scores'] < cutoff]\n",
        "print('Number of Anomalous Data Points Found:', len(anomalies))\n",
        "anomalies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ryjbCT8kY6Pf",
        "outputId": "be60321b-a504-4570-a9cd-81faeba35c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Anomalous Data Points Found: 27\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    city               country  meal_inexpensive  \\\n",
              "21              New York         United States             25.00   \n",
              "28                London        United Kingdom             18.45   \n",
              "35           Los Angeles         United States             20.00   \n",
              "74                 Miami         United States             20.00   \n",
              "103            Singapore             Singapore             11.09   \n",
              "116               Boston         United States             20.00   \n",
              "157            San Diego         United States             20.00   \n",
              "190             Brooklyn         United States             19.00   \n",
              "272             San Jose         United States             19.00   \n",
              "347              Sharjah  United Arab Emirates              5.45   \n",
              "536             Hamilton               Bermuda             29.00   \n",
              "589           Kermanshah                  Iran              4.13   \n",
              "990              Oakland         United States             17.50   \n",
              "1234           Asheville         United States             15.00   \n",
              "1321              Irvine         United States             20.00   \n",
              "1417            Chandler         United States             15.00   \n",
              "1419         Jersey City         United States             15.00   \n",
              "1804     Fort Lauderdale         United States             20.00   \n",
              "1841    Rancho Cucamonga         United States             16.00   \n",
              "2017           Sunnyvale         United States             20.00   \n",
              "2057            Bellevue         United States             30.00   \n",
              "2609        Santa Monica         United States             20.00   \n",
              "2661       Mountain View         United States             17.00   \n",
              "2742  Walton upon Thames        United Kingdom             28.91   \n",
              "3748        Walnut Creek         United States             15.00   \n",
              "3769           Rockville         United States             17.00   \n",
              "3810         Chapel Hill         United States             15.00   \n",
              "\n",
              "      meal_midrange  mcdonalds  beer_domestic_restaurant  \\\n",
              "21           100.00      10.00                      7.20   \n",
              "28            79.95       8.61                      7.26   \n",
              "35            88.59      10.00                      8.00   \n",
              "74           100.00      10.00                      6.00   \n",
              "103           66.55       5.92                      7.39   \n",
              "116           95.00      10.00                      7.00   \n",
              "157           80.00      10.00                      7.00   \n",
              "190           87.50      12.00                      8.00   \n",
              "272           80.00       9.04                      7.00   \n",
              "347           19.06       6.81                      6.81   \n",
              "536          150.00      15.00                      9.00   \n",
              "589           14.03       3.28                      0.83   \n",
              "990           90.00      10.25                      7.00   \n",
              "1234          65.00       7.50                      4.75   \n",
              "1321         100.00       8.00                      8.00   \n",
              "1417          55.00       6.00                      5.00   \n",
              "1419          80.00      10.00                      7.00   \n",
              "1804          80.00       9.00                      6.50   \n",
              "1841          75.00       8.00                      7.50   \n",
              "2017          80.00      10.00                      6.25   \n",
              "2057          80.00       9.00                      6.00   \n",
              "2609          82.50       9.99                      8.00   \n",
              "2661         100.00      10.00                      6.00   \n",
              "2742          86.10       6.15                      6.77   \n",
              "3748          82.50       8.00                      6.00   \n",
              "3769          80.00       8.00                      6.00   \n",
              "3810          66.25       8.00                      4.50   \n",
              "\n",
              "      beer_imported_restaurant  capuccino  coke  water_restaurant  ...  jeans  \\\n",
              "21                       10.00       5.38  2.72              2.32  ...  62.13   \n",
              "28                        6.15       4.14  2.12              1.69  ...  89.10   \n",
              "35                        8.00       4.94  2.70              2.30  ...  59.39   \n",
              "74                        8.00       4.59  2.19              2.03  ...  47.71   \n",
              "103                       8.87       4.55  1.49              1.06  ...  76.31   \n",
              "116                       8.00       4.83  2.32              2.10  ...  52.61   \n",
              "157                       8.00       4.82  2.40              2.07  ...  53.95   \n",
              "190                       8.00       4.88  2.11              1.67  ...  59.45   \n",
              "272                       9.00       5.17  2.53              2.29  ...  44.50   \n",
              "347                       6.81       4.63  0.78              0.31  ...  45.21   \n",
              "536                      10.00       5.61  3.00              2.43  ...  90.75   \n",
              "589                       1.00       2.49  0.51              0.33  ...  56.14   \n",
              "990                       7.00       4.73  2.42              1.72  ...  55.44   \n",
              "1234                      6.00       4.43  1.83              1.62  ...  53.75   \n",
              "1321                      7.50       5.00  2.67              1.90  ...  51.85   \n",
              "1417                      5.88       4.88  2.00              1.25  ...  43.75   \n",
              "1419                      7.00       3.78  2.17              2.32  ...  50.75   \n",
              "1804                      7.00       4.86  2.38              1.77  ...  48.00   \n",
              "1841                      8.50       5.33  2.50              2.38  ...  41.33   \n",
              "2017                      6.50       4.50  4.25              1.25  ...  40.00   \n",
              "2057                      7.00       5.11  2.33              1.78  ...  48.87   \n",
              "2609                      8.00       5.00  2.70              2.30  ...  58.75   \n",
              "2661                      8.00       4.67  2.00              1.43  ...  42.25   \n",
              "2742                      6.15       3.79  1.75              1.43  ...  79.64   \n",
              "3748                      8.00       4.83  3.00              2.02  ...  57.00   \n",
              "3769                      7.50       4.05  2.17              1.79  ...  56.75   \n",
              "3810                      5.50       4.45  2.12              1.51  ...  40.75   \n",
              "\n",
              "      summer_dress  nike_shoes  leather_shoes  apt_1bed_center  \\\n",
              "21           57.42       95.29         145.62          3851.51   \n",
              "28           40.65       91.32         104.69          2417.07   \n",
              "35           40.75       93.93         133.00          2527.29   \n",
              "74           40.36       79.44         105.77          2604.55   \n",
              "103          44.52      108.41         115.29          2734.54   \n",
              "116          41.43       83.83         113.69          2715.62   \n",
              "157          42.06       89.62          99.95          2576.61   \n",
              "190          40.00       89.23         128.57          2806.77   \n",
              "272          39.38       78.15         106.25          2469.60   \n",
              "347          59.70       68.23          66.93          6678.30   \n",
              "536          55.00      142.50         141.33          3250.00   \n",
              "589          75.37      271.81          49.52         12608.83   \n",
              "990          45.88       97.86         107.50          2400.00   \n",
              "1234         36.80       84.25         136.14          2700.00   \n",
              "1321         33.33       87.50         134.29          2575.77   \n",
              "1417         29.62       79.17          93.00          3080.33   \n",
              "1419         55.80       77.50          80.62          2800.00   \n",
              "1804         37.38       68.78          89.29          2381.25   \n",
              "1841         21.67       64.67          96.43          2500.00   \n",
              "2017         66.25      105.00         145.00          2950.00   \n",
              "2057         32.99       94.33         105.00          2500.00   \n",
              "2609         40.75       89.00         132.50          2650.00   \n",
              "2661         35.00       67.50          72.50          3075.00   \n",
              "2742         39.97       59.45          74.83          3075.00   \n",
              "3748         36.33       78.75          96.67          2566.67   \n",
              "3769         35.60       75.62         111.00          2450.00   \n",
              "3810         32.50       80.45          91.00          3000.00   \n",
              "\n",
              "      avg_monthly_salary  mortgage_interest  data_quality  anomaly_scores  \\\n",
              "21               5944.31               5.40             1       -0.283824   \n",
              "28               4033.84               2.97             1       -0.157279   \n",
              "35               5456.15               5.18             1       -0.176469   \n",
              "74               4195.86               5.33             1       -0.191275   \n",
              "103              4619.45               2.50             1       -0.210719   \n",
              "116              5342.19               5.02             1       -0.209758   \n",
              "157              5924.47               4.94             1       -0.184758   \n",
              "190              4950.19               5.19             1       -0.221866   \n",
              "272              6808.52               5.22             1       -0.170092   \n",
              "347              1681.97               4.44             1       -0.324067   \n",
              "536              5973.81               6.50             1       -0.270156   \n",
              "589               492.79              19.33             0       -0.336991   \n",
              "990              6111.11               5.43             1       -0.151432   \n",
              "1234             3480.00               5.43             1       -0.203543   \n",
              "1321             6306.19               5.19             1       -0.184758   \n",
              "1417             3817.94               5.38             0       -0.255703   \n",
              "1419             6100.00               5.43             1       -0.220890   \n",
              "1804             4948.00               4.87             1       -0.150991   \n",
              "1841             4389.00               5.46             0       -0.173273   \n",
              "2017             7125.00               5.43             1       -0.245694   \n",
              "2057             7833.33               5.18             1       -0.173273   \n",
              "2609             6512.00               5.45             0       -0.199745   \n",
              "2661             7404.00               5.43             0       -0.255192   \n",
              "2742             2952.00               3.08             1       -0.255192   \n",
              "3748             5600.00               4.95             0       -0.185685   \n",
              "3769             6020.83               5.36             0       -0.165573   \n",
              "3810             5500.00               5.32             0       -0.246199   \n",
              "\n",
              "      anomaly  \n",
              "21          1  \n",
              "28          1  \n",
              "35          1  \n",
              "74          1  \n",
              "103         1  \n",
              "116         1  \n",
              "157         1  \n",
              "190         1  \n",
              "272         1  \n",
              "347         1  \n",
              "536         1  \n",
              "589         1  \n",
              "990         1  \n",
              "1234        1  \n",
              "1321        1  \n",
              "1417        1  \n",
              "1419        1  \n",
              "1804        1  \n",
              "1841        1  \n",
              "2017        1  \n",
              "2057        1  \n",
              "2609        1  \n",
              "2661        1  \n",
              "2742        1  \n",
              "3748        1  \n",
              "3769        1  \n",
              "3810        1  \n",
              "\n",
              "[27 rows x 55 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6c94b03-d5e8-4a7d-b09d-014acb8212c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>city</th>\n",
              "      <th>country</th>\n",
              "      <th>meal_inexpensive</th>\n",
              "      <th>meal_midrange</th>\n",
              "      <th>mcdonalds</th>\n",
              "      <th>beer_domestic_restaurant</th>\n",
              "      <th>beer_imported_restaurant</th>\n",
              "      <th>capuccino</th>\n",
              "      <th>coke</th>\n",
              "      <th>water_restaurant</th>\n",
              "      <th>...</th>\n",
              "      <th>jeans</th>\n",
              "      <th>summer_dress</th>\n",
              "      <th>nike_shoes</th>\n",
              "      <th>leather_shoes</th>\n",
              "      <th>apt_1bed_center</th>\n",
              "      <th>avg_monthly_salary</th>\n",
              "      <th>mortgage_interest</th>\n",
              "      <th>data_quality</th>\n",
              "      <th>anomaly_scores</th>\n",
              "      <th>anomaly</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>New York</td>\n",
              "      <td>United States</td>\n",
              "      <td>25.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>7.20</td>\n",
              "      <td>10.00</td>\n",
              "      <td>5.38</td>\n",
              "      <td>2.72</td>\n",
              "      <td>2.32</td>\n",
              "      <td>...</td>\n",
              "      <td>62.13</td>\n",
              "      <td>57.42</td>\n",
              "      <td>95.29</td>\n",
              "      <td>145.62</td>\n",
              "      <td>3851.51</td>\n",
              "      <td>5944.31</td>\n",
              "      <td>5.40</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.283824</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>London</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>18.45</td>\n",
              "      <td>79.95</td>\n",
              "      <td>8.61</td>\n",
              "      <td>7.26</td>\n",
              "      <td>6.15</td>\n",
              "      <td>4.14</td>\n",
              "      <td>2.12</td>\n",
              "      <td>1.69</td>\n",
              "      <td>...</td>\n",
              "      <td>89.10</td>\n",
              "      <td>40.65</td>\n",
              "      <td>91.32</td>\n",
              "      <td>104.69</td>\n",
              "      <td>2417.07</td>\n",
              "      <td>4033.84</td>\n",
              "      <td>2.97</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.157279</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Los Angeles</td>\n",
              "      <td>United States</td>\n",
              "      <td>20.00</td>\n",
              "      <td>88.59</td>\n",
              "      <td>10.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>4.94</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.30</td>\n",
              "      <td>...</td>\n",
              "      <td>59.39</td>\n",
              "      <td>40.75</td>\n",
              "      <td>93.93</td>\n",
              "      <td>133.00</td>\n",
              "      <td>2527.29</td>\n",
              "      <td>5456.15</td>\n",
              "      <td>5.18</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.176469</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>Miami</td>\n",
              "      <td>United States</td>\n",
              "      <td>20.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>4.59</td>\n",
              "      <td>2.19</td>\n",
              "      <td>2.03</td>\n",
              "      <td>...</td>\n",
              "      <td>47.71</td>\n",
              "      <td>40.36</td>\n",
              "      <td>79.44</td>\n",
              "      <td>105.77</td>\n",
              "      <td>2604.55</td>\n",
              "      <td>4195.86</td>\n",
              "      <td>5.33</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.191275</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>Singapore</td>\n",
              "      <td>Singapore</td>\n",
              "      <td>11.09</td>\n",
              "      <td>66.55</td>\n",
              "      <td>5.92</td>\n",
              "      <td>7.39</td>\n",
              "      <td>8.87</td>\n",
              "      <td>4.55</td>\n",
              "      <td>1.49</td>\n",
              "      <td>1.06</td>\n",
              "      <td>...</td>\n",
              "      <td>76.31</td>\n",
              "      <td>44.52</td>\n",
              "      <td>108.41</td>\n",
              "      <td>115.29</td>\n",
              "      <td>2734.54</td>\n",
              "      <td>4619.45</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.210719</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>Boston</td>\n",
              "      <td>United States</td>\n",
              "      <td>20.00</td>\n",
              "      <td>95.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>4.83</td>\n",
              "      <td>2.32</td>\n",
              "      <td>2.10</td>\n",
              "      <td>...</td>\n",
              "      <td>52.61</td>\n",
              "      <td>41.43</td>\n",
              "      <td>83.83</td>\n",
              "      <td>113.69</td>\n",
              "      <td>2715.62</td>\n",
              "      <td>5342.19</td>\n",
              "      <td>5.02</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.209758</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>San Diego</td>\n",
              "      <td>United States</td>\n",
              "      <td>20.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>4.82</td>\n",
              "      <td>2.40</td>\n",
              "      <td>2.07</td>\n",
              "      <td>...</td>\n",
              "      <td>53.95</td>\n",
              "      <td>42.06</td>\n",
              "      <td>89.62</td>\n",
              "      <td>99.95</td>\n",
              "      <td>2576.61</td>\n",
              "      <td>5924.47</td>\n",
              "      <td>4.94</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.184758</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>United States</td>\n",
              "      <td>19.00</td>\n",
              "      <td>87.50</td>\n",
              "      <td>12.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>4.88</td>\n",
              "      <td>2.11</td>\n",
              "      <td>1.67</td>\n",
              "      <td>...</td>\n",
              "      <td>59.45</td>\n",
              "      <td>40.00</td>\n",
              "      <td>89.23</td>\n",
              "      <td>128.57</td>\n",
              "      <td>2806.77</td>\n",
              "      <td>4950.19</td>\n",
              "      <td>5.19</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.221866</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>San Jose</td>\n",
              "      <td>United States</td>\n",
              "      <td>19.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>9.04</td>\n",
              "      <td>7.00</td>\n",
              "      <td>9.00</td>\n",
              "      <td>5.17</td>\n",
              "      <td>2.53</td>\n",
              "      <td>2.29</td>\n",
              "      <td>...</td>\n",
              "      <td>44.50</td>\n",
              "      <td>39.38</td>\n",
              "      <td>78.15</td>\n",
              "      <td>106.25</td>\n",
              "      <td>2469.60</td>\n",
              "      <td>6808.52</td>\n",
              "      <td>5.22</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.170092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>Sharjah</td>\n",
              "      <td>United Arab Emirates</td>\n",
              "      <td>5.45</td>\n",
              "      <td>19.06</td>\n",
              "      <td>6.81</td>\n",
              "      <td>6.81</td>\n",
              "      <td>6.81</td>\n",
              "      <td>4.63</td>\n",
              "      <td>0.78</td>\n",
              "      <td>0.31</td>\n",
              "      <td>...</td>\n",
              "      <td>45.21</td>\n",
              "      <td>59.70</td>\n",
              "      <td>68.23</td>\n",
              "      <td>66.93</td>\n",
              "      <td>6678.30</td>\n",
              "      <td>1681.97</td>\n",
              "      <td>4.44</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.324067</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>Hamilton</td>\n",
              "      <td>Bermuda</td>\n",
              "      <td>29.00</td>\n",
              "      <td>150.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>9.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>5.61</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.43</td>\n",
              "      <td>...</td>\n",
              "      <td>90.75</td>\n",
              "      <td>55.00</td>\n",
              "      <td>142.50</td>\n",
              "      <td>141.33</td>\n",
              "      <td>3250.00</td>\n",
              "      <td>5973.81</td>\n",
              "      <td>6.50</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.270156</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>589</th>\n",
              "      <td>Kermanshah</td>\n",
              "      <td>Iran</td>\n",
              "      <td>4.13</td>\n",
              "      <td>14.03</td>\n",
              "      <td>3.28</td>\n",
              "      <td>0.83</td>\n",
              "      <td>1.00</td>\n",
              "      <td>2.49</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.33</td>\n",
              "      <td>...</td>\n",
              "      <td>56.14</td>\n",
              "      <td>75.37</td>\n",
              "      <td>271.81</td>\n",
              "      <td>49.52</td>\n",
              "      <td>12608.83</td>\n",
              "      <td>492.79</td>\n",
              "      <td>19.33</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.336991</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>Oakland</td>\n",
              "      <td>United States</td>\n",
              "      <td>17.50</td>\n",
              "      <td>90.00</td>\n",
              "      <td>10.25</td>\n",
              "      <td>7.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>4.73</td>\n",
              "      <td>2.42</td>\n",
              "      <td>1.72</td>\n",
              "      <td>...</td>\n",
              "      <td>55.44</td>\n",
              "      <td>45.88</td>\n",
              "      <td>97.86</td>\n",
              "      <td>107.50</td>\n",
              "      <td>2400.00</td>\n",
              "      <td>6111.11</td>\n",
              "      <td>5.43</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.151432</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>Asheville</td>\n",
              "      <td>United States</td>\n",
              "      <td>15.00</td>\n",
              "      <td>65.00</td>\n",
              "      <td>7.50</td>\n",
              "      <td>4.75</td>\n",
              "      <td>6.00</td>\n",
              "      <td>4.43</td>\n",
              "      <td>1.83</td>\n",
              "      <td>1.62</td>\n",
              "      <td>...</td>\n",
              "      <td>53.75</td>\n",
              "      <td>36.80</td>\n",
              "      <td>84.25</td>\n",
              "      <td>136.14</td>\n",
              "      <td>2700.00</td>\n",
              "      <td>3480.00</td>\n",
              "      <td>5.43</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.203543</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1321</th>\n",
              "      <td>Irvine</td>\n",
              "      <td>United States</td>\n",
              "      <td>20.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>7.50</td>\n",
              "      <td>5.00</td>\n",
              "      <td>2.67</td>\n",
              "      <td>1.90</td>\n",
              "      <td>...</td>\n",
              "      <td>51.85</td>\n",
              "      <td>33.33</td>\n",
              "      <td>87.50</td>\n",
              "      <td>134.29</td>\n",
              "      <td>2575.77</td>\n",
              "      <td>6306.19</td>\n",
              "      <td>5.19</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.184758</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1417</th>\n",
              "      <td>Chandler</td>\n",
              "      <td>United States</td>\n",
              "      <td>15.00</td>\n",
              "      <td>55.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>5.88</td>\n",
              "      <td>4.88</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.25</td>\n",
              "      <td>...</td>\n",
              "      <td>43.75</td>\n",
              "      <td>29.62</td>\n",
              "      <td>79.17</td>\n",
              "      <td>93.00</td>\n",
              "      <td>3080.33</td>\n",
              "      <td>3817.94</td>\n",
              "      <td>5.38</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.255703</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1419</th>\n",
              "      <td>Jersey City</td>\n",
              "      <td>United States</td>\n",
              "      <td>15.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>3.78</td>\n",
              "      <td>2.17</td>\n",
              "      <td>2.32</td>\n",
              "      <td>...</td>\n",
              "      <td>50.75</td>\n",
              "      <td>55.80</td>\n",
              "      <td>77.50</td>\n",
              "      <td>80.62</td>\n",
              "      <td>2800.00</td>\n",
              "      <td>6100.00</td>\n",
              "      <td>5.43</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.220890</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1804</th>\n",
              "      <td>Fort Lauderdale</td>\n",
              "      <td>United States</td>\n",
              "      <td>20.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>9.00</td>\n",
              "      <td>6.50</td>\n",
              "      <td>7.00</td>\n",
              "      <td>4.86</td>\n",
              "      <td>2.38</td>\n",
              "      <td>1.77</td>\n",
              "      <td>...</td>\n",
              "      <td>48.00</td>\n",
              "      <td>37.38</td>\n",
              "      <td>68.78</td>\n",
              "      <td>89.29</td>\n",
              "      <td>2381.25</td>\n",
              "      <td>4948.00</td>\n",
              "      <td>4.87</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.150991</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1841</th>\n",
              "      <td>Rancho Cucamonga</td>\n",
              "      <td>United States</td>\n",
              "      <td>16.00</td>\n",
              "      <td>75.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>7.50</td>\n",
              "      <td>8.50</td>\n",
              "      <td>5.33</td>\n",
              "      <td>2.50</td>\n",
              "      <td>2.38</td>\n",
              "      <td>...</td>\n",
              "      <td>41.33</td>\n",
              "      <td>21.67</td>\n",
              "      <td>64.67</td>\n",
              "      <td>96.43</td>\n",
              "      <td>2500.00</td>\n",
              "      <td>4389.00</td>\n",
              "      <td>5.46</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.173273</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017</th>\n",
              "      <td>Sunnyvale</td>\n",
              "      <td>United States</td>\n",
              "      <td>20.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>6.25</td>\n",
              "      <td>6.50</td>\n",
              "      <td>4.50</td>\n",
              "      <td>4.25</td>\n",
              "      <td>1.25</td>\n",
              "      <td>...</td>\n",
              "      <td>40.00</td>\n",
              "      <td>66.25</td>\n",
              "      <td>105.00</td>\n",
              "      <td>145.00</td>\n",
              "      <td>2950.00</td>\n",
              "      <td>7125.00</td>\n",
              "      <td>5.43</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.245694</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2057</th>\n",
              "      <td>Bellevue</td>\n",
              "      <td>United States</td>\n",
              "      <td>30.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>9.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>7.00</td>\n",
              "      <td>5.11</td>\n",
              "      <td>2.33</td>\n",
              "      <td>1.78</td>\n",
              "      <td>...</td>\n",
              "      <td>48.87</td>\n",
              "      <td>32.99</td>\n",
              "      <td>94.33</td>\n",
              "      <td>105.00</td>\n",
              "      <td>2500.00</td>\n",
              "      <td>7833.33</td>\n",
              "      <td>5.18</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.173273</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2609</th>\n",
              "      <td>Santa Monica</td>\n",
              "      <td>United States</td>\n",
              "      <td>20.00</td>\n",
              "      <td>82.50</td>\n",
              "      <td>9.99</td>\n",
              "      <td>8.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>5.00</td>\n",
              "      <td>2.70</td>\n",
              "      <td>2.30</td>\n",
              "      <td>...</td>\n",
              "      <td>58.75</td>\n",
              "      <td>40.75</td>\n",
              "      <td>89.00</td>\n",
              "      <td>132.50</td>\n",
              "      <td>2650.00</td>\n",
              "      <td>6512.00</td>\n",
              "      <td>5.45</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.199745</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2661</th>\n",
              "      <td>Mountain View</td>\n",
              "      <td>United States</td>\n",
              "      <td>17.00</td>\n",
              "      <td>100.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>4.67</td>\n",
              "      <td>2.00</td>\n",
              "      <td>1.43</td>\n",
              "      <td>...</td>\n",
              "      <td>42.25</td>\n",
              "      <td>35.00</td>\n",
              "      <td>67.50</td>\n",
              "      <td>72.50</td>\n",
              "      <td>3075.00</td>\n",
              "      <td>7404.00</td>\n",
              "      <td>5.43</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.255192</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2742</th>\n",
              "      <td>Walton upon Thames</td>\n",
              "      <td>United Kingdom</td>\n",
              "      <td>28.91</td>\n",
              "      <td>86.10</td>\n",
              "      <td>6.15</td>\n",
              "      <td>6.77</td>\n",
              "      <td>6.15</td>\n",
              "      <td>3.79</td>\n",
              "      <td>1.75</td>\n",
              "      <td>1.43</td>\n",
              "      <td>...</td>\n",
              "      <td>79.64</td>\n",
              "      <td>39.97</td>\n",
              "      <td>59.45</td>\n",
              "      <td>74.83</td>\n",
              "      <td>3075.00</td>\n",
              "      <td>2952.00</td>\n",
              "      <td>3.08</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.255192</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3748</th>\n",
              "      <td>Walnut Creek</td>\n",
              "      <td>United States</td>\n",
              "      <td>15.00</td>\n",
              "      <td>82.50</td>\n",
              "      <td>8.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>4.83</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.02</td>\n",
              "      <td>...</td>\n",
              "      <td>57.00</td>\n",
              "      <td>36.33</td>\n",
              "      <td>78.75</td>\n",
              "      <td>96.67</td>\n",
              "      <td>2566.67</td>\n",
              "      <td>5600.00</td>\n",
              "      <td>4.95</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.185685</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3769</th>\n",
              "      <td>Rockville</td>\n",
              "      <td>United States</td>\n",
              "      <td>17.00</td>\n",
              "      <td>80.00</td>\n",
              "      <td>8.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>7.50</td>\n",
              "      <td>4.05</td>\n",
              "      <td>2.17</td>\n",
              "      <td>1.79</td>\n",
              "      <td>...</td>\n",
              "      <td>56.75</td>\n",
              "      <td>35.60</td>\n",
              "      <td>75.62</td>\n",
              "      <td>111.00</td>\n",
              "      <td>2450.00</td>\n",
              "      <td>6020.83</td>\n",
              "      <td>5.36</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.165573</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3810</th>\n",
              "      <td>Chapel Hill</td>\n",
              "      <td>United States</td>\n",
              "      <td>15.00</td>\n",
              "      <td>66.25</td>\n",
              "      <td>8.00</td>\n",
              "      <td>4.50</td>\n",
              "      <td>5.50</td>\n",
              "      <td>4.45</td>\n",
              "      <td>2.12</td>\n",
              "      <td>1.51</td>\n",
              "      <td>...</td>\n",
              "      <td>40.75</td>\n",
              "      <td>32.50</td>\n",
              "      <td>80.45</td>\n",
              "      <td>91.00</td>\n",
              "      <td>3000.00</td>\n",
              "      <td>5500.00</td>\n",
              "      <td>5.32</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.246199</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27 rows × 55 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6c94b03-d5e8-4a7d-b09d-014acb8212c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6c94b03-d5e8-4a7d-b09d-014acb8212c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6c94b03-d5e8-4a7d-b09d-014acb8212c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After experimenting, we decided to go with a cutoff of -0.15, as this contained the outliers. At the same time, given our dataset size and quality, we didn't want to remove too many records, and many of these records were in the US, which we know is realistically more expensive based on the fact that the US has large major cities. We mark the anomalies with a data quality of 0, as well as the city of Sharjah, because its price is significantly higher than that of other cities."
      ],
      "metadata": {
        "id": "XdahFKZLZE_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mark anomalies\n",
        "bad_anomalies = anomalies[anomalies['data_quality'] == 0]\n",
        "bad_anomalies = pd.concat([bad_anomalies,\n",
        "                          anomalies[anomalies['city'] == 'Sharjah']])\n",
        "data_clean.loc[bad_anomalies.index, 'anomaly'] = -1"
      ],
      "metadata": {
        "id": "FZlcDaEDZCy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize these removed anomalies."
      ],
      "metadata": {
        "id": "hcarxIuxZeq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(data_clean.index, data_clean['apt_1bed_center'], s=4,\n",
        "            c=data_clean['anomaly'])\n",
        "plt.ylabel('Apt Price')\n",
        "plt.xlabel('Data Point')\n",
        "plt.title('Anomalies Removed')\n",
        "plt.colorbar(label='Anomaly (-1 = Anomalous)', orientation='horizontal')"
      ],
      "metadata": {
        "id": "pnKhp8A_ZgYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We drop these outliers from the dataset. Also, before training, let's drop the city and country features since there are many unique categorical values for these features, and we are more concerned with the actual prices of items rather than predicting based on country or city. Finally, let's drop the **\"data_quality\"** column, since it has no effect on our actual data."
      ],
      "metadata": {
        "id": "ExfVc56MZkWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean = data_clean[data_clean['anomaly'] != -1]\n",
        "data_clean = data_clean.drop(['anomaly_scores', 'anomaly',\n",
        "                              'city', 'country', 'data_quality'], axis=1)"
      ],
      "metadata": {
        "id": "LAS2Hfo6ZnxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKPD4UNvR5xc"
      },
      "source": [
        "#Naive Bayes Model\n",
        "\n",
        "\n",
        "In the Naive Bayes mode, we computed a log transformation on label values to apply a normaly distribution to the y values. Most of the y-values fall within a certain range that was encompassed mostly within 1 bin despite variations in total bin sizes. transform y into log y since y is not a normal distribution.It is recommended there is atleast 10 members per bin but we had two instances of bins with only 3 and 1 members. When we tested the naive bayes model by binning the y values (one-bedroom apartment prices in the city) into 3, 5 and 10 bins and performing cross validation, binning the labels into 3 values perf0rmed better with an average score of 70.5% compared to labels binned into 5 and 10 bins with average accuracies of 65.7% and 50%. This is indicates that it is harder to predict which price range a one-bedroom partment will be classified as when the range is small.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7KFGotxSF9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71b4e9aa-a430-4cfc-c684-23bb57e59c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 3 bins: 0.7419089002097692\n",
            "Accuracy with 5 bins: 0.6242033762860852\n",
            "Accuracy with 10 bins: 0.4337428828288882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=10.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "X = data_clean.drop('apt_1bed_center', axis=1)\n",
        "y = data_clean['apt_1bed_center']\n",
        "y_transformed = np.log(y)\n",
        "y_transformed = y_transformed.to_numpy()\n",
        "\n",
        "# Reshape y_array into a column vector\n",
        "y_array_reshaped = y_transformed.reshape(-1, 1)\n",
        "\n",
        "# Create a discretizer object with 3, 5 and 10 bins\n",
        "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
        "discretizer2 = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
        "discretizer3 = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
        "\n",
        "# Fit and transform the target variable y\n",
        "y_discrete = discretizer.fit_transform(y_array_reshaped)\n",
        "y_discrete2 = discretizer2.fit_transform(y_array_reshaped)\n",
        "y_discrete3 = discretizer3.fit_transform(y_array_reshaped)\n",
        "\n",
        "# Convert the output back to a 1D array\n",
        "y_discrete = y_discrete.ravel()\n",
        "y_discrete2 = y_discrete2.ravel()\n",
        "y_discrete3 = y_discrete3.ravel()\n",
        "\n",
        "\n",
        "# Create naives bayes model\n",
        "naive_bayes = GaussianNB()\n",
        "features = X\n",
        "\n",
        "#Cross Validation\n",
        "scores = cross_val_score(naive_bayes,X=features,y=y_discrete, cv = 10)\n",
        "scores2 = cross_val_score(naive_bayes,X=features,y=y_discrete2, cv = 10)\n",
        "scores3 = cross_val_score(naive_bayes,X=features,y=y_discrete3, cv = 10)\n",
        "\n",
        "print(\"Accuracy with 3 bins:\", scores.mean())\n",
        "print(\"Accuracy with 5 bins:\", scores2.mean())\n",
        "print(\"Accuracy with 10 bins:\", scores3.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHJDmnwioqxg"
      },
      "source": [
        "It seems evident that the highest accuracy is one where you have to predict with the least number of bins. We fit the naive bayes classifiers to the various bins and print utilize the predict function to make a classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eAHTnKUd_PL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "naive_bayes_3 = naive_bayes.fit(X = features, y = y_discrete)\n",
        "naive_bayes_5 = naive_bayes.fit(X = features, y = y_discrete2)\n",
        "naive_bayes_10 = naive_bayes.fit(X = features, y = y_discrete3)\n",
        "\n",
        "naive_bayes_predict = cross_val_predict(naive_bayes,X=features,y=y_discrete, cv = 10)\n",
        "print(\"Confusion Matrix with 3 bins:\")\n",
        "print(confusion_matrix(y_discrete, naive_bayes_predict))\n",
        "print(\"Classification Report with 3 bins:\")\n",
        "print(classification_report(y_discrete, naive_bayes_predict))\n",
        "\n",
        "naive_bayes_predict2 = cross_val_predict(naive_bayes,X=features,y=y_discrete2, cv = 10)\n",
        "print(\"Confusion Matrix with 5 bins:\")\n",
        "print(confusion_matrix(y_discrete2, naive_bayes_predict2))\n",
        "print(\"Classification Report with 5 bins:\")\n",
        "print(classification_report(y_discrete2, naive_bayes_predict2))\n",
        "\n",
        "naive_bayes_predict3 = cross_val_predict(naive_bayes,X=features,y=y_discrete3, cv = 10)\n",
        "print(\"Confusion Matrix with 10 bins:\")\n",
        "print(confusion_matrix(y_discrete2, naive_bayes_predict3))\n",
        "print(\"Classification Report with 10 bins:\")\n",
        "print(classification_report(y_discrete2, naive_bayes_predict3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo4xGmKVqzJK"
      },
      "source": [
        "The roc_curve function does not support multiclass classification. Since there are more than two classes/bins we cannot use roc_curve directly.One possible solution is to use the One-vs-Rest (OvR) strategy to convert the multiclass problem into several binary classification problems. In OvR, for each class, a binary classifier is trained to distinguish samples belonging to that class from the rest of the samples. Then, the outputs of these binary classifiers can be used to compute the ROC curve and AUC for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z086Z5llqRZx"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#separate test and training data\n",
        "test_train3 = train_test_split(features, y_discrete, test_size=.2, train_size=.8, random_state = 33)\n",
        "test_train5 = train_test_split(features, y_discrete2, test_size=.2, train_size=.8, random_state = 33)\n",
        "test_train10 = train_test_split(features, y_discrete3, test_size=.2, train_size=.8, random_state = 33)\n",
        "\n",
        "# create a One-vs-Rest classifier with GaussianNB as the base classifier\n",
        "ovr_nb3 = OneVsRestClassifier(GaussianNB())\n",
        "ovr_nb5 = OneVsRestClassifier(GaussianNB())\n",
        "ovr_nb10 = OneVsRestClassifier(GaussianNB())\n",
        "# fit the One-vs-Rest classifier on the training data\n",
        "ovr_nb3.fit(test_train3[0], test_train3[2])\n",
        "ovr_nb5.fit(test_train5[0], test_train5[2])\n",
        "ovr_nb10.fit(test_train10[0], test_train10[2])\n",
        "\n",
        "# predict the probabilities for each class using the One-vs-Rest classifier\n",
        "probabilities3 = ovr_nb3.predict_proba(test_train3[1])\n",
        "probabilities5 = ovr_nb5.predict_proba(test_train5[1])\n",
        "probabilities10 = ovr_nb10.predict_proba(test_train10[1])\n",
        "\n",
        "# compute the ROC curve and AUC for each class\n",
        "fpr3 = dict()\n",
        "tpr3 = dict()\n",
        "auc3 = dict()\n",
        "for i in range(len(ovr_nb3.classes_)):\n",
        "    fpr3[i], tpr3[i], _ = roc_curve(test_train3[3], probabilities3[:, i], pos_label=i)\n",
        "    auc3[i] = roc_auc_score(test_train3[3] == i, probabilities3[:, i])\n",
        "\n",
        "fpr5 = dict()\n",
        "tpr5 = dict()\n",
        "auc5 = dict()\n",
        "for i in range(len(ovr_nb5.classes_)):\n",
        "    fpr5[i], tpr5[i], _ = roc_curve(test_train5[3], probabilities5[:, i], pos_label=i)\n",
        "    auc5[i] = roc_auc_score(test_train5[3] == i, probabilities5[:, i])\n",
        "\n",
        "fpr10 = dict()\n",
        "tpr10 = dict()\n",
        "auc10 = dict()\n",
        "for i in range(len(ovr_nb10.classes_)):\n",
        "    fpr10[i], tpr10[i], _ = roc_curve(test_train10[3], probabilities10[:, i], pos_label=i)\n",
        "    auc10[i] = roc_auc_score(test_train10[3] == i, probabilities10[:, i])\n",
        "\n",
        "for i in range(len(ovr_nb3.classes_)):\n",
        "    plt.plot(fpr3[i], tpr3[i], label=f'class {i} (AUC = {auc3[i]:.2f})')\n",
        "\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('fpr')\n",
        "plt.ylabel('tpr')\n",
        "plt.title('ROC Curve Naive Bayes with 3 Bins')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "for i in range(len(ovr_nb5.classes_)):\n",
        "    plt.plot(fpr5[i], tpr5[i], label=f'class {i} (AUC = {auc5[i]:.2f})')\n",
        "\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('fpr')\n",
        "plt.ylabel('tpr')\n",
        "plt.title('ROC Curve Naive Bayes with 5 Bins')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "for i in range(len(ovr_nb10.classes_)):\n",
        "    plt.plot(fpr10[i], tpr10[i], label=f'class {i} (AUC = {auc10[i]:.2f})')\n",
        "\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel('fpr')\n",
        "plt.ylabel('tpr')\n",
        "plt.title('ROC Curve Naive Bayes with 10 Bins')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1IBfxDTU4jS"
      },
      "source": [
        "# Random Over Sampler\n",
        "Instead of manually transforming the data to make up for class imbalance, let's utilize the ROS package and bin the range into 5 groups. This duplicates minority class records. With ROS, we see a 10% increase in accuracy of the classifier and through cross validation as opposed to manually transforming the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMz4KcyE68qv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Instantiate and fit Random Over Sampler object\n",
        "ros = RandomOverSampler()\n",
        "y_whole = np.round(y).astype(int)\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y_whole)\n",
        "\n",
        "y_whole = y_resampled.to_numpy().reshape(-1,1)\n",
        "# Discretize both the training and testing sets\n",
        "discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
        "discretizer_ytrain = discretizer.fit_transform(y_whole)\n",
        "discretizer_ytrain = discretizer_ytrain.ravel()\n",
        "\n",
        "naive_bayes0 = GaussianNB()\n",
        "features = X_resampled\n",
        "scores_0 = cross_val_score(naive_bayes0,X=features,y=discretizer_ytrain, cv = 10)\n",
        "print(\"Cross Validation Accuracy with 5 bins:\", scores_0.mean())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KCtfHg2wOP5"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.histograms import histogram\n",
        "print(np.unique(test_train3[2]))\n",
        "print(np.unique(y_discrete3, return_counts= True))\n",
        "plt.hist(np.log(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L73G0Kk5TD_"
      },
      "source": [
        "#K Nearest Neighbors\n",
        "\n",
        "The knn pipeline is built using the StandardScaler and PCA classes from sklearn.preprocessing and the KNeighborsClassifier class from sklearn.neighbors.\n",
        "The code then defines a parameter grid using the param_grid dictionary that contains the hyperparameters for PCA and KNeighborsClassifier. It specifies the number of components to keep after PCA and the number of neighbors to use for k-NN. Afterwards, a grid search is utilized to find the best hyperparameters for the pipeline. The following cell involves knn with binning of 5 and ROS data. With the optimal number of bins and ROS data, the best number of neighbors is 1 with an accuracy of 98% however this is likely due to overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Oya-mp0wV0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a080c1a3-903e-48c9-af80-e20cb640026b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best number of dimensions with 5 bins: 0.9\n",
            "Best number of neighbors with 5 bins: 1\n",
            "Best accuracy with 5 bins: 0.9802020202020202\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "scaler = StandardScaler()\n",
        "pca = PCA(svd_solver = 'full')\n",
        "knn = KNeighborsClassifier()\n",
        "pipeline = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('knn', knn)])\n",
        "\n",
        "param_grid = {\n",
        "    'pca__n_components': [0.7, 0.8, 0.9, 0.95],\n",
        "    'knn__n_neighbors': list(range(1, 25))\n",
        "}\n",
        "\n",
        "grid_search5 = GridSearchCV(pipeline, param_grid, cv=5)\n",
        "grid_search5.fit(features, discretizer_ytrain)\n",
        "\n",
        "print(\"Best number of dimensions with 5 bins:\", grid_search5.best_params_['pca__n_components'])\n",
        "print(\"Best number of neighbors with 5 bins:\", grid_search5.best_params_['knn__n_neighbors'])\n",
        "print(\"Best accuracy with 5 bins:\", grid_search5.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we perform knn for 3, 5 and 10 bins without ROS, we get significantly lower accuracy scores with an almost 30% decrease."
      ],
      "metadata": {
        "id": "ivwZ0RyYKF0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3ol4LZT1vAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28dd3951-2bb8-411e-d392-4e417b953d88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 3 discrete bins:   0.8130766933758025\n",
            "Accuracy for 5 discrete bins:   0.7002861693126958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 10 discrete bins:   0.4485691534365202\n"
          ]
        }
      ],
      "source": [
        "scores = cross_val_score(pipeline, X, y_discrete, cv=5)\n",
        "print(\"Accuracy for 3 discrete bins:  \", scores.mean())\n",
        "\n",
        "scores2 = cross_val_score(pipeline, X, y_discrete2, cv=5)\n",
        "print(\"Accuracy for 5 discrete bins:  \", scores2.mean())\n",
        "\n",
        "scores3 = cross_val_score(pipeline, X, y_discrete3, cv=5)\n",
        "print(\"Accuracy for 10 discrete bins:  \", scores3.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MLoVuMoQrpt"
      },
      "source": [
        "#KNN Without Binning\n",
        "We had to bin in the Naive Bayes model but let's try KNN without binning. When we do a knn regressor with ROS, we don't get accuracies as high as knn with binning wih the best number of beighbors = 7 and an accuracy of 59%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb_YXYXrRKmY"
      },
      "outputs": [],
      "source": [
        "# grid_search_0 = GridSearchCV(pipeline, param_grid, cv=5)\n",
        "# grid_search_0.fit(features, y)\n",
        "# print(\"Best number of dimensions with 3 bins:\", grid_search_0.best_params_['pca__n_components'])\n",
        "# print(\"Best number of neighbors with 3 bins:\", grid_search_0.best_params_['knn__n_neighbors'])\n",
        "# print(\"Best accuracy with 3 bins:\", grid_search_0.best_score_)\n",
        "# scores0 = cross_val_score(pipeline, X, y_whole, cv=5)\n",
        "# print(\"Accuracy for 5 discrete bins:  \", scores0.mean())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "for n in range(1, 11):\n",
        "    knn = KNeighborsRegressor(n_neighbors=n)\n",
        "    knn.fit(X_train, y_train)\n",
        "    scores = cross_val_score(knn, X, y, cv=5)\n",
        "    print(\"KNN without binning with\", n, \"neightbors:\", scores.mean())\n",
        "\n"
      ],
      "metadata": {
        "id": "2tuYk10q8bQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iz6-RoRl_nX"
      },
      "source": [
        "# **Support Vector Regression**\n",
        "\n",
        "To build the svr model, we used a pipeline using the StandardScaler and PCA classes again. Using a parameter grid that adjusts the hyperparameters for PCA and SVR to find the optimal values. We then use cross_val_predict to perform nested cross validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjekAtP1mBb1"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "pca = PCA()\n",
        "svr = SVR()\n",
        "pipeline = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('svr', svr)])\n",
        "param_grid = {\n",
        "    'pca__n_components': list(range(5, 47)),\n",
        "    'svr__kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "oned_y = np.ravel(y)\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, error_score='raise')\n",
        "grid_search.fit(X, oned_y)\n",
        "print(\"Best number of dimensions: \", grid_search.best_params_['pca__n_components'])\n",
        "print(\"Best kernel type: \", grid_search.best_params_['svr__kernel'])\n",
        "pred = cross_val_predict(grid_search, X, oned_y, cv=5)\n",
        "mse = mean_squared_error(y, pred)\n",
        "r2 = r2_score(y, pred)\n",
        "print(\"Mean Squared Error: \", mse)\n",
        "print(\"R-Squared: \", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oclPPHQqQdGf"
      },
      "source": [
        "Using the optimal hyperparameters we found above, lets visualize our model by looking at the SVR with a few different features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUEYMbBwQfEa"
      },
      "outputs": [],
      "source": [
        "xscaler = StandardScaler()\n",
        "yscaler = StandardScaler()\n",
        "cap = data_clean[['capuccino']]\n",
        "cap = xscaler.fit_transform(cap)\n",
        "y = yscaler.fit_transform(y)\n",
        "svr = SVR(kernel='linear')\n",
        "svr.fit(cap, y)\n",
        "\n",
        "plt.scatter(cap, y, color = 'red')\n",
        "plt.plot(cap, svr.predict(cap), color = 'blue')\n",
        "plt.title('Support Vector Regression Model')\n",
        "plt.xlabel('Price of a capuccino (scaled)')\n",
        "plt.ylabel('Price of a 1 bedroom apartment (scaled)')\n",
        "plt.show()\n",
        "\n",
        "toyota = data_clean[['toyota_new']]\n",
        "toyota = xscaler.fit_transform(toyota)\n",
        "svr.fit(toyota, y)\n",
        "\n",
        "plt.scatter(toyota, y, color = 'red')\n",
        "plt.plot(toyota, svr.predict(toyota), color = 'blue')\n",
        "plt.title('Support Vector Regression Model')\n",
        "plt.xlabel('Price of a new toyota (scaled)')\n",
        "plt.ylabel('Price of a 1 bedroom apartment (scaled)')\n",
        "plt.show()\n",
        "\n",
        "eggs = data_clean[['eggs']]\n",
        "eggs = xscaler.fit_transform(eggs)\n",
        "svr.fit(eggs, y)\n",
        "\n",
        "plt.scatter(eggs, y, color = 'red')\n",
        "plt.plot(eggs, svr.predict(eggs), color = 'blue')\n",
        "plt.title('Support Vector Regression Model')\n",
        "plt.xlabel('Price of eggs (scaled)')\n",
        "plt.ylabel('Price of a 1 bedroom apartment (scaled)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9iSL0TAQlY6"
      },
      "source": [
        "# SVC\n",
        "\n",
        "Our labels are continous, meaning that we had to use SVR instead of SVC with the original data. Let's see how using binning to turn this into a classification problem changes things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7TMt3hnQuzJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "y_transformed = np.log(y)\n",
        "y_transformed = y_transformed.to_numpy()\n",
        "\n",
        "# Reshape y_array into a column vector\n",
        "y_array_reshaped = y_transformed.reshape(-1, 1)\n",
        "\n",
        "# Create a discretizer object with 3, 5 and 10 bins\n",
        "discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')\n",
        "discretizer2 = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
        "discretizer3 = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform')\n",
        "\n",
        "# Fit and transform the target variable y\n",
        "y_discrete = discretizer.fit_transform(y_array_reshaped)\n",
        "y_discrete2 = discretizer2.fit_transform(y_array_reshaped)\n",
        "y_discrete3 = discretizer3.fit_transform(y_array_reshaped)\n",
        "\n",
        "# Convert the output back to a 1D array\n",
        "y_discrete = y_discrete.ravel()\n",
        "y_discrete2 = y_discrete2.ravel()\n",
        "y_discrete3 = y_discrete3.ravel()\n",
        "\n",
        "\n",
        "# Create naives bayes model\n",
        "svc = SVC()\n",
        "features = X\n",
        "\n",
        "#Cross Validation\n",
        "scores = cross_val_score(svc, X=features, y=y_discrete, cv = 10)\n",
        "scores2 = cross_val_score(svc, X=features, y=y_discrete2, cv = 10)\n",
        "scores3 = cross_val_score(svc,X=features,y=y_discrete3, cv = 10)\n",
        "\n",
        "print(\"Accuracy with 3 bins:\", scores.mean())\n",
        "print(\"Accuracy with 5 bins:\", scores2.mean())\n",
        "print(\"Accuracy with 10 bins:\", scores3.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXLawvUqQxMw"
      },
      "source": [
        "Once again, we find that using a smaller amount of bins results in a more accurate classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_OOXFIZu-ZC"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-2ynFl9l8tS"
      },
      "source": [
        "The code imports the necessary libraries, including pandas for data manipulation and Linear Regression, train_test_split and RFE for feature selection. The data is split into training and testing sets, and the RFE (Recursive Feature Elimination) method is used to select the top five features that are most relevant to the target variable (rent). The RFE method works by recursively removing the least important feature and fitting the model again until the desired number of features is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdXJ-w13wobL"
      },
      "outputs": [],
      "source": [
        "#LINEAR REGRESSION\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "X_ln = data_clean.drop('apt_1bed_center', axis=1)\n",
        "y_ln = data_clean['apt_1bed_center']\n",
        "X_train_ln, X_test_ln, y_train_ln, y_test_ln = train_test_split(X_ln, y_ln, test_size=0.2, random_state=42)\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "rfe = RFE(model, n_features_to_select=5)\n",
        "rfe.fit(X_train_ln, y_train_ln)\n",
        "\n",
        "print(\"Feature ranking:\", rfe.ranking_)\n",
        "\n",
        "ranking = rfe.ranking_\n",
        "\n",
        "\n",
        "feature_ranking = {}\n",
        "for i in range(len(X_ln.columns)):\n",
        "    feature_ranking[X_ln.columns[i]] = ranking[i]\n",
        "\n",
        "sorted_features = sorted(feature_ranking.items(), key=lambda x: x[1])\n",
        "print(\"sorted features \", sorted_features)\n",
        "\n",
        "for feature, rank in sorted_features:\n",
        "    print(f\"{feature}: {rank}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WITICcZpmTmq"
      },
      "source": [
        "The code keeps track of the highest R-squared value and the best set of features found so far. The best_features list is updated with the best set of features found in each iteration, and the highest_r_squared variable is updated with the highest R-squared value found.\n",
        "\n",
        "Overall, this code is an example of a systematic approach to feature selection using the RFE method and a Linear Regression model. The resulting set of features can then be used to build a more accurate predictive model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiFalyqS9CSO"
      },
      "outputs": [],
      "source": [
        "X_train_ln, X_test_ln, y_train_ln, y_test_ln = train_test_split(X_ln, y_ln, test_size=0.2, random_state=42)\n",
        "\n",
        "highest_r_squared = 0.0\n",
        "best_features = []\n",
        "\n",
        "while len(X_train_ln.columns) > 4:\n",
        "    model = LinearRegression()\n",
        "\n",
        "    rfe = RFE(model, n_features_to_select=5)\n",
        "    rfe.fit(X_train_ln, y_train_ln)\n",
        "\n",
        "    print(\"Feature ranking:\", rfe.ranking_)\n",
        "\n",
        "    ranking = rfe.ranking_\n",
        "\n",
        "    feature_ranking = {}\n",
        "    for i in range(len(X_train_ln.columns)):\n",
        "        feature_ranking[X_train_ln.columns[i]] = ranking[i]\n",
        "\n",
        "    sorted_features = sorted(feature_ranking.items(), key=lambda x: x[1])\n",
        "\n",
        "    best_features_for_iteration = [sorted_features[i][0] for i in range(5)]\n",
        "\n",
        "    model.fit(X_train_ln[best_features_for_iteration], y_train_ln)\n",
        "    r_squared = model.score(X_test_ln[best_features_for_iteration], y_test_ln)\n",
        "\n",
        "    if r_squared > highest_r_squared:\n",
        "        highest_r_squared = r_squared\n",
        "        best_features = best_features_for_iteration\n",
        "\n",
        "    print(\"Best features for iteration:\", best_features_for_iteration)\n",
        "    print(\"R-squared for iteration:\", r_squared)\n",
        "    print(\"Current best features:\", best_features)\n",
        "    print(\"Current highest R-squared:\", highest_r_squared)\n",
        "    print(\"\")\n",
        "\n",
        "    X_train_ln = X_train_ln.drop(best_features_for_iteration, axis=1)\n",
        "    X_test_ln = X_test_ln.drop(best_features_for_iteration, axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG0hCdjB9Jrx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "X_ln = data_clean.drop('apt_1bed_center', axis=1)\n",
        "y_ln = data_clean['apt_1bed_center']\n",
        "\n",
        "r2_scores = []\n",
        "\n",
        "for train_index, test_index in kf.split(X_ln):\n",
        "\n",
        "    X_train_ln, X_test_ln = X_ln.iloc[train_index], X_ln.iloc[test_index]\n",
        "    y_train_ln, y_test_ln = y_ln.iloc[train_index], y_ln.iloc[test_index]\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_ln, y_train_ln)\n",
        "\n",
        "    y_pred_ln = model.predict(X_test_ln)\n",
        "    r2 = r2_score(y_test_ln, y_pred_ln)\n",
        "    print(\" R-squared value: \", r2)\n",
        "\n",
        "    r2_scores.append(r2)\n",
        "\n",
        "print(\"Average R-squared value:\", np.mean(r2_scores))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdJWaKu3AdQq"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np\n",
        "\n",
        "scaler = StandardScaler()\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "\n",
        "pipeline = Pipeline(steps=[('scaler', scaler), ('lin_reg', lin_reg)])\n",
        "\n",
        "param_grid = {\n",
        "    'lin_reg__fit_intercept': [True, False],\n",
        "    'lin_reg__copy_X': [True, False]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='r2')\n",
        "grid_search.fit(X_ln, y_ln)\n",
        "\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_ln = best_model.predict(X_ln)\n",
        "r2 = r2_score(y_ln, y_pred_ln)\n",
        "print(\"Train R-squared:\", r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NQ-tylSr39A"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "median_rent = data_clean['apt_1bed_center'].median()\n",
        "data_clean['above_median_rent'] = (data_clean['apt_1bed_center'] > median_rent).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_reg = data_clean.drop(['above_median_rent'], axis=1)\n",
        "y_reg = data_clean['above_median_rent']\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a logistic regression model\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "y_pred_reg = clf.predict(X_test_reg)\n",
        "accuracy = accuracy_score(y_test_reg, y_pred_reg)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is too high for Logistic Regression. An accuracy that is too high could be a sign of overfitting. Overfitting occurs when a model is too complex and fits the training data too closely, leading to poor performance on unseen data. In the case of a logistic regression model, an accuracy that is too high could mean that the model is relying too much on the training data and is not generalizing well to new data. This can be problematic because the model may not perform well on real-world data that it was not trained on."
      ],
      "metadata": {
        "id": "Iyq8TiW8Lu9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "median_rent = data_clean['apt_1bed_center'].median()\n",
        "data_clean['above_median_rent'] = (data_clean['apt_1bed_center'] > median_rent).astype(int)\n",
        "\n",
        "X_reg = data_clean.drop(['above_median_rent'], axis=1)\n",
        "y_reg = data_clean['above_median_rent']\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_reg = scaler.fit_transform(X_train_reg)\n",
        "X_test_reg = scaler.transform(X_test_reg)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "scores = cross_val_score(clf, X_train_reg, y_train_reg, cv=5)\n",
        "print(f\"Cross validation scores: {scores}\")\n",
        "print(f\"Mean accuracy: {scores.mean()}\")\n",
        "\n",
        "clf.fit(X_train_reg, y_train_reg)\n",
        "y_pred_reg = clf.predict(X_test_reg)\n",
        "accuracy = accuracy_score(y_test_reg, y_pred_reg)\n",
        "print(f\"Accuracy on testing data: {accuracy}\")\n"
      ],
      "metadata": {
        "id": "jfQbC-rDLkHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the logistic regression model\n",
        "clf = LogisticRegression(C=1.0, max_iter=1000, solver='lbfgs')\n",
        "\n",
        "# Perform k-fold cross-validation\n",
        "cv_scores_train = cross_val_score(clf, X_train_reg, y_train_reg, cv=5, scoring='accuracy')\n",
        "cv_scores_test = cross_val_score(clf, X_test_reg, y_test_reg, cv=5, scoring='accuracy')\n",
        "\n",
        "# Calculate the average train and test scores\n",
        "train_score = np.mean(cv_scores_train)\n",
        "test_score = np.mean(cv_scores_test)\n",
        "\n",
        "# Print the scores\n",
        "print(f\"Average train score: {train_score}\")\n",
        "print(f\"Average test score: {test_score}\")\n"
      ],
      "metadata": {
        "id": "j0S4V_SiLrru"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}